# Neural Network Cost Minimization & Activation Analysis  

This project explores neural networks by implementing a basic feedforward network and optimizing its cost function. It compares the effects of different activation functions (Sigmoid, ReLU) and hidden layer structures on performance.  

## Features  
- **Forward & Backpropagation:** Implements cost function minimization using gradient descent.  
- **Activation Functions:** Compares Sigmoid and ReLU activations.  
- **Hidden Layers:** Tests performance with and without additional hidden layers.  
- **Visualization:** Plots cost reduction over iterations.  

## How It Works  
1. **Data Normalization:** Scales input data for better performance.  
2. **Training:** Adjusts weights iteratively to minimize cost.  
3. **Evaluation:** Compares results across different activation functions and architectures.  

## Results  
- **Sigmoid vs. ReLU:** ReLU tends to converge faster and avoids vanishing gradients.  
- **Hidden Layers:** Adding layers can improve learning but may require tuning.  
- **Cost Reduction:** Visualized using matplotlib to track progress.  

## Run It Yourself  
Make sure you have Python and NumPy installed, then run the scripts to see the training process in action!  

---

A simple but effective deep learning experiment ðŸš€  
